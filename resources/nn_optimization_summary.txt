Neural Network Training Optimization for Smoother Convergence
================================================================

APPROACH: CONSERVATIVE & STABLE
--------------------------------

CHANGES MADE (v2 - Stability First):
-------------------------------------

1. ARCHITECTURE:
   - Simple network: [64, 32] (proven stable)
   - Higher dropout: 0.4 for strong regularization
   - BatchNorm momentum: 0.01 (very slow updates = very smooth)

2. OPTIMIZER & LEARNING RATE:
   - SGD with Nesterov momentum (0.9)
     * More stable than Adam for small datasets
     * Smoother convergence trajectory
   - Very low LR: 0.0005 (slow but steady)
   - ExponentialLR decay: gamma=0.98 (smooth 2% decay per epoch)

3. REGULARIZATION (Strong):
   - High L2: 0.01 (prevent overfitting)
   - Moderate label smoothing: 0.1
   - Aggressive gradient clipping: max_norm=0.5

4. TRAINING DYNAMICS:
   - Batch size: 32 (proven stable)
   - High patience: 30 epochs
   - Class weights for imbalance

WHY THIS WORKS:
---------------
✓ SGD + Momentum: More stable than Adam, smoother updates
✓ Low LR: Prevents oscillations, ensures smooth descent
✓ Exponential decay: Gradual, predictable LR reduction
✓ High dropout: Strong regularization = less overfitting
✓ Slow BatchNorm: Running stats update very gradually
✓ Aggressive grad clipping: Prevents any gradient spikes

EXPECTED RESULTS:
-----------------
✓ Very smooth validation curves (minimal oscillation)
✓ Gradual, steady improvement
✓ Train/val curves close together (less overfitting)
✓ Stable convergence without wild swings
✓ May take longer but will be much smoother

TRADE-OFFS:
-----------
- Slower convergence (but smoother)
- May need more epochs to reach peak performance
- Conservative approach prioritizes stability over speed
