Neural Network Training Optimization for Smoother Convergence
================================================================

CHANGES MADE:
-------------

1. ARCHITECTURE:
   - Deeper network: [128, 64, 32] (was [64, 32])
   - Lower dropout: 0.2 (was 0.3) for less noise
   - BatchNorm momentum: 0.1 for smoother running stats

2. LEARNING RATE & OPTIMIZATION:
   - Higher initial LR: 0.003 (was 0.001)
   - Cosine Annealing with Warm Restarts scheduler
     * Smooth LR decay with periodic restarts
     * T_0=10 epochs, T_mult=2 (10, 20, 40...)
     * Replaces ReduceLROnPlateau for smoother curves
   - AdamW optimizer with standard betas (0.9, 0.999)

3. REGULARIZATION:
   - Light L2: 0.0005 (was 0.001)
   - Light label smoothing: 0.05 (was 0.1)
   - Gradient clipping: max_norm=1.0 for stability

4. TRAINING DYNAMICS:
   - Larger batch size: 64 (was 32) for stable gradients
   - More patience: 25 epochs (was 20)
   - Better gradient flow with He initialization

EXPECTED RESULTS:
-----------------
✓ Smoother loss curves (no jagged spikes)
✓ Better convergence (higher final accuracy)
✓ More stable validation metrics
✓ Reduced overfitting with deeper network
✓ Faster initial learning with higher LR
✓ Periodic LR restarts help escape local minima

WHY THESE CHANGES:
------------------
- CosineAnnealingWarmRestarts: Provides smooth LR decay with periodic 
  "warm restarts" that help escape local minima
- Larger batches: More stable gradient estimates = smoother training
- Lower dropout: Less noise during training = smoother curves
- Higher initial LR: Faster convergence in early epochs
- Light regularization: Allows model to learn better while preventing overfitting

NEXT STEPS:
-----------
Run training and observe:
1. Smoother accuracy curves (less oscillation)
2. Smoother loss curves (gradual descent)
3. Better final test accuracy
4. Validation curves tracking training more closely
