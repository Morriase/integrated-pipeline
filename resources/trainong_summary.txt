================================================================================
SMC MODEL TRAINING - FULL DATASET APPROACH
================================================================================

ðŸŽ¯ Training Strategy:
  - Train on ALL symbols combined (unified dataset)
  - 2,500+ samples (vs 178-295 per symbol)
  - 3 models total (vs 44 models)
  - Better generalization, less overfitting
  - ATR normalization makes features comparable
ðŸ” Detected Kaggle environment
  âœ“ Using cleaned data (optimized for Neural Network)
ðŸ“‚ Data Directory: /kaggle/working/Data-output-clean
ðŸ“‚ Model Output Directory: /kaggle/working/Model-output

ðŸ” Checking data availability...
  âœ“ Train data: /kaggle/working/Data-output-clean/processed_smc_data_train_clean.csv
  âœ“ Val data:   /kaggle/working/Data-output-clean/processed_smc_data_val_clean.csv
  âœ“ Test data:  /kaggle/working/Data-output-clean/processed_smc_data_test_clean.csv

ðŸ“Š Dataset Statistics:
  Train samples: 10,050
  Val samples:   2,159
  Test samples:  2,129
  Total:         14,338
  Symbols:       11
  Features:      78

================================================================================
STARTING UNIFIED TRAINING
================================================================================

################################################################################
# Training All Models on UNIFIED DATASET
################################################################################

ðŸŽ¯ Strategy: Train on ALL data for maximum generalization
   Core Models: RandomForest, XGBoost, NeuralNetwork
   LSTM: Disabled (use --include-lstm to enable)

ðŸŒ² Starting RandomForest training...

================================================================================
Training Random Forest on FULL UNIFIED DATASET
================================================================================

ðŸ“Š Training on FULL dataset:
  Train: 10,050 samples (11 symbols)
  Val:   2,159 samples
  Test:  2,129 samples
  Selected 53 features
  Remapped labels: [0 1]

ðŸ”„ Performing 5-fold cross-validation...

ðŸ”„ Performing 5-fold cross-validation...

ðŸŒ² Training Random Forest for UNIFIED...
  Training samples: 8,040
  Features: 53

  Train accuracy: 0.650
  Val accuracy:   0.663
  Train-Val gap:  -0.013
  âœ… Good generalization (gap â‰¤ 15%)

  Feature importance computed (feature names not available)
  Fold 1/5: Accuracy=0.663, F1=0.625

ðŸŒ² Training Random Forest for UNIFIED...
  Training samples: 8,040
  Features: 53

  Train accuracy: 0.654
  Val accuracy:   0.648
  Train-Val gap:  0.006
  âœ… Good generalization (gap â‰¤ 15%)

  Feature importance computed (feature names not available)
  Fold 2/5: Accuracy=0.648, F1=0.610

ðŸŒ² Training Random Forest for UNIFIED...
  Training samples: 8,040
  Features: 53

  Train accuracy: 0.652
  Val accuracy:   0.659
  Train-Val gap:  -0.008
  âœ… Good generalization (gap â‰¤ 15%)

  Feature importance computed (feature names not available)
  Fold 3/5: Accuracy=0.659, F1=0.622

ðŸŒ² Training Random Forest for UNIFIED...
  Training samples: 8,040
  Features: 53

  Train accuracy: 0.652
  Val accuracy:   0.658
  Train-Val gap:  -0.006
  âœ… Good generalization (gap â‰¤ 15%)

  Feature importance computed (feature names not available)
  Fold 4/5: Accuracy=0.658, F1=0.621

ðŸŒ² Training Random Forest for UNIFIED...
  Training samples: 8,040
  Features: 53

  Train accuracy: 0.657
  Val accuracy:   0.638
  Train-Val gap:  0.019
  âœ… Good generalization (gap â‰¤ 15%)

  Feature importance computed (feature names not available)
  Fold 5/5: Accuracy=0.638, F1=0.603

  Cross-Validation Results:
    Mean Accuracy: 0.653 Â± 0.009
    Min Accuracy:  0.638
    Max Accuracy:  0.663
    Range:         0.025
    Mean F1-Score: 0.616 Â± 0.008
    âœ… STABLE: Std dev 0.009 â‰¤ 0.10

    Poor-Performing Folds (below mean - std):
      Fold 5: 0.638 (deviation: -0.015)

ðŸ“ˆ Cross-Validation Results:
  Mean Accuracy: 0.6532 Â± 0.0091
  Stability: âœ… Stable

ðŸŽ¯ Training final model...

ðŸŒ² Training Random Forest for UNIFIED...
  Training samples: 10,050
  Features: 53

  ðŸ”„ Performing cross-validation...

ðŸ”„ Performing 5-fold cross-validation...

ðŸŒ² Training Random Forest for UNIFIED...
  Training samples: 8,040
  Features: 53

  Train accuracy: 0.650
  Val accuracy:   0.663
  Train-Val gap:  -0.013
  âœ… Good generalization (gap â‰¤ 15%)

  Feature importance computed (feature names not available)
  Fold 1/5: Accuracy=0.663, F1=0.625

ðŸŒ² Training Random Forest for UNIFIED...
  Training samples: 8,040
  Features: 53

  Train accuracy: 0.654
  Val accuracy:   0.648
  Train-Val gap:  0.006
  âœ… Good generalization (gap â‰¤ 15%)

  Feature importance computed (feature names not available)
  Fold 2/5: Accuracy=0.648, F1=0.610

ðŸŒ² Training Random Forest for UNIFIED...
  Training samples: 8,040
  Features: 53

  Train accuracy: 0.652
  Val accuracy:   0.659
  Train-Val gap:  -0.008
  âœ… Good generalization (gap â‰¤ 15%)

  Feature importance computed (feature names not available)
  Fold 3/5: Accuracy=0.659, F1=0.622

ðŸŒ² Training Random Forest for UNIFIED...
  Training samples: 8,040
  Features: 53

  Train accuracy: 0.652
  Val accuracy:   0.658
  Train-Val gap:  -0.006
  âœ… Good generalization (gap â‰¤ 15%)

  Feature importance computed (feature names not available)
  Fold 4/5: Accuracy=0.658, F1=0.621

ðŸŒ² Training Random Forest for UNIFIED...
  Training samples: 8,040
  Features: 53

  Train accuracy: 0.657
  Val accuracy:   0.638
  Train-Val gap:  0.019
  âœ… Good generalization (gap â‰¤ 15%)

  Feature importance computed (feature names not available)
  Fold 5/5: Accuracy=0.638, F1=0.603

  Cross-Validation Results:
    Mean Accuracy: 0.653 Â± 0.009
    Min Accuracy:  0.638
    Max Accuracy:  0.663
    Range:         0.025
    Mean F1-Score: 0.616 Â± 0.008
    âœ… STABLE: Std dev 0.009 â‰¤ 0.10

    Poor-Performing Folds (below mean - std):
      Fold 5: 0.638 (deviation: -0.015)

  Train accuracy: 0.653
  Val accuracy:   0.661
  Train-Val gap:  -0.008
  âœ… Good generalization (gap â‰¤ 15%)

  Top 10 Most Important Features:
    TBM_Bars_to_Hit                         : 0.2863
    TBM_Reward_Per_Trade_ATR                : 0.1346
    TBM_Risk_Per_Trade_ATR                  : 0.0980
    FVG_Depth_ATR_ZScore                    : 0.0795
    FVG_Quality_Fuzzy                       : 0.0764
    FVG_Depth_ATR                           : 0.0653
    FVG_Size_Fuzzy_Score                    : 0.0595
    FVG_Distance_to_Price_ATR_ZScore        : 0.0468
    FVG_Distance_to_Price_ATR               : 0.0446
    FVG_Mitigated                           : 0.0428

ðŸ“Š Evaluating on Validation set...

  Classification Metrics:
    Accuracy: 0.661
    Precision (macro): 0.662
    Recall (macro): 0.746
    F1-Score (macro): 0.630

  Trading Metrics (1:2.0 R:R):
    Win Rate: 100.0% (1077/1077 trades)
    Profit Factor: inf
    Expected Value/Trade: 2.00R
    Trade Accuracy: 100.0%
    Timeouts: 1082 (50.1%)
    ðŸ’° PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Win
  True Loss         400        49
  True Win          682      1028

  Overfitting Analysis:
    Training Accuracy: 0.653
    Validation Accuracy: 0.661
    Train-Val Gap: -0.008
    âœ… No significant overfitting

ðŸ“Š Evaluating on Test set...

  Classification Metrics:
    Accuracy: 0.652
    Precision (macro): 0.665
    Recall (macro): 0.730
    F1-Score (macro): 0.631

  Trading Metrics (1:2.0 R:R):
    Win Rate: 100.0% (1014/1014 trades)
    Profit Factor: inf
    Expected Value/Trade: 2.00R
    Trade Accuracy: 100.0%
    Timeouts: 1115 (52.4%)
    ðŸ’° PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Win
  True Loss         437        62
  True Win          678       952

ðŸ’¾ Model pickle saved to /kaggle/working/Model-output/UNIFIED_RandomForest.pkl
ðŸ’¾ Metadata saved to /kaggle/working/Model-output/UNIFIED_RandomForest_metadata.json

âœ… Model save complete: /kaggle/working/Model-output
âœ… RandomForest completed in 15.6s

ðŸš€ Starting XGBoost training...

================================================================================
Training XGBoost on FULL UNIFIED DATASET
================================================================================

ðŸ“Š Training on FULL dataset:
  Train: 10,050 samples (11 symbols)
  Val:   2,159 samples
  Test:  2,129 samples
  Selected 53 features
  Remapped labels: [0 1]

ðŸŽ¯ Training with aggressive regularization...

ðŸš€ Training XGBoost for UNIFIED...
  Training samples: 10,050
  Features: 53

  Train accuracy: 0.712
  Val accuracy:   0.693
  Best iteration: 199

  Top 10 Most Important Features:
    TBM_Bars_to_Hit                         : 0.1923
    TBM_Risk_Per_Trade_ATR                  : 0.0954
    TBM_Reward_Per_Trade_ATR                : 0.0928
    FVG_Mitigated                           : 0.0730
    FVG_Depth_ATR                           : 0.0704
    FVG_Quality_Fuzzy                       : 0.0411
    FVG_Size_Fuzzy_Score                    : 0.0386
    FVG_Distance_to_Price_ATR               : 0.0342
    OB_Bounce_Quality                       : 0.0291
    OB_Bullish_Valid                        : 0.0276

ðŸ“Š Evaluating on Validation set...

  Classification Metrics:
    Accuracy: 0.693
    Precision (macro): 0.667
    Recall (macro): 0.751
    F1-Score (macro): 0.653

  Trading Metrics (1:2.0 R:R):
    Win Rate: 100.0% (1181/1181 trades)
    Profit Factor: inf
    Expected Value/Trade: 2.00R
    Trade Accuracy: 100.0%
    Timeouts: 978 (45.3%)
    ðŸ’° PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Win
  True Loss         382        67
  True Win          596      1114

  Overfitting Analysis:
    Training Accuracy: 0.712
    Validation Accuracy: 0.693
    Train-Val Gap: 0.019
    âœ… No significant overfitting

ðŸ“Š Evaluating on Test set...

  Classification Metrics:
    Accuracy: 0.699
    Precision (macro): 0.682
    Recall (macro): 0.752
    F1-Score (macro): 0.669

  Trading Metrics (1:2.0 R:R):
    Win Rate: 100.0% (1137/1137 trades)
    Profit Factor: inf
    Expected Value/Trade: 2.00R
    Trade Accuracy: 100.0%
    Timeouts: 992 (46.6%)
    ðŸ’° PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Win
  True Loss         425        74
  True Win          567      1063

ðŸ’¾ Model pickle saved to /kaggle/working/Model-output/UNIFIED_XGBoost.pkl
ðŸ’¾ Metadata saved to /kaggle/working/Model-output/UNIFIED_XGBoost_metadata.json

âœ… Model save complete: /kaggle/working/Model-output
âœ… XGBoost completed in 1.0s

ðŸ§  Starting Neural Network training...

================================================================================
Training Neural Network on FULL UNIFIED DATASET
================================================================================

ðŸ“Š Training on FULL dataset:
  Train: 10,050 samples (11 symbols)
  Val:   2,159 samples
  Test:  2,129 samples
  Selected 53 features
  Remapped labels: [0 1]

ðŸŽ¯ Training with simplified architecture...

ðŸ§  Training Neural Network for UNIFIED...
  Device: cuda
  Training samples: 10,050
  Features: 53
  âš ï¸ WARNING: 1 constant features detected!
     These features will not help learning.
  Architecture: 53 -> 64 -> 32 -> 3

  Class distribution:
       0:   2028 ( 20.2%)
       1:   8022 ( 79.8%)

  Detected cleaned binary data (0, 1)
  Will map predictions back to original labels (-1, 1)

  Using 2-class classification
  Training label mapping: {0: 0, 1: 1}
  Prediction label mapping: {0: -1, 1: 1}
  Class distribution: {0: 2028, 1: 8022}
  Class weights: {0: 2.477810650887574, 1: 0.6264023934181002}
  Epoch 10/200 - LR: 0.001000 - Train Loss: 0.6134, Train Acc: 0.716 | Val Loss: 0.5942, Val Acc: 0.707
  Epoch 20/200 - LR: 0.000500 - Train Loss: 0.5976, Train Acc: 0.704 | Val Loss: 0.5945, Val Acc: 0.730
  Epoch 30/200 - LR: 0.000250 - Train Loss: 0.5894, Train Acc: 0.711 | Val Loss: 0.6122, Val Acc: 0.780
  Epoch 40/200 - LR: 0.000125 - Train Loss: 0.5935, Train Acc: 0.714 | Val Loss: 0.5946, Val Acc: 0.740
  Epoch 50/200 - LR: 0.000063 - Train Loss: 0.5899, Train Acc: 0.720 | Val Loss: 0.5866, Val Acc: 0.705

  Early stopping at epoch 54 (patience=20)
Learning curves saved to: /kaggle/working/Training_Images/UNIFIED_NN_learning_curves.png
Overfitting metrics saved to: /kaggle/working/Training_Images/UNIFIED_NN_overfitting_metrics.json

============================================================
OVERFITTING MONITOR SUMMARY
============================================================
Status: âœ“ HEALTHY
Train-Val Gap: -1.79% (Threshold: 15%)
Total Epochs: 54
Training Duration: 0:00:53.000033

Final Metrics:
  Train Accuracy: 0.7190
  Val Accuracy:   0.7369
  Train Loss:     0.5906
  Val Loss:       0.5903

Best Epoch (Highest Val Accuracy):
  Epoch:          29
  Train Accuracy: 0.7111
  Val Accuracy:   0.7800
  Train Loss:     0.5894
  Val Loss:       0.6122
============================================================


  Final Train Accuracy: 0.719
  Final Val Accuracy:   0.737
  Train-Val Gap:        -0.018 (-1.8%)

ðŸ“Š Evaluating on Validation set...

  Classification Metrics:
    Accuracy: 0.570
    Precision (macro): 0.311
    Recall (macro): 0.240
    F1-Score (macro): 0.271

  Trading Metrics (1:2.0 R:R):
    Win Rate: 61.1% (1320/2159 trades)
    Profit Factor: 3.15
    Expected Value/Trade: 0.83R
    Trade Accuracy: 72.0%
    ðŸ’° PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Timeout  Pred Win
  True Loss           0           0         0
  True Timeout      360           0        89
  True Win          479           0      1231

ðŸ“Š Evaluating on Test set...

  Classification Metrics:
    Accuracy: 0.535
    Precision (macro): 0.306
    Recall (macro): 0.233
    F1-Score (macro): 0.265

  Trading Metrics (1:2.0 R:R):
    Win Rate: 58.2% (1240/2129 trades)
    Profit Factor: 2.79
    Expected Value/Trade: 0.75R
    Trade Accuracy: 69.9%
    ðŸ’° PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Timeout  Pred Win
  True Loss           0           0         0
  True Timeout      398           0       101
  True Win          491           0      1139

ðŸ’¾ Model pickle saved to /kaggle/working/Model-output/UNIFIED_NeuralNetwork.pkl
ðŸ’¾ Metadata saved to /kaggle/working/Model-output/UNIFIED_NeuralNetwork_metadata.json
ðŸ’¾ Scaler saved to /kaggle/working/Model-output/UNIFIED_NeuralNetwork_scaler.pkl

âœ… Model save complete: /kaggle/working/Model-output
âœ… Neural Network completed in 54.7s

================================================================================
Training Summary:
  Total Duration: 71.3s (1.2 min)
  Successful Models: 3/3
  Failed Models: 0/3
================================================================================

================================================================================
All training completed in 71.3s (1.2 minutes)
================================================================================

================================================================================
TRAINING SUMMARY REPORT - UNIFIED DATASET
================================================================================

ðŸ“Š Model Performance:
  Model                Train Acc    Val Acc      Test Acc     Gap        Status              
  ----------------------------------------------------------------------------------------------------
  RandomForest         0.653        0.661        0.652        -0.82%     âœ… Good              
  XGBoost              0.712        0.693        0.699        1.90%      âœ… Good              
  NeuralNetwork        0.000        0.570        0.535        -57.02%    âœ… Good              

ðŸ’¾ Results saved to: /kaggle/working/Model-output/training_results.json

âœ… Training complete!

================================================================================
NEXT STEPS:
================================================================================
1. Review training results in models/trained/training_results.json
2. Check model files: UNIFIED_RandomForest.pkl, UNIFIED_XGBoost.pkl, UNIFIED_NeuralNetwork.pkl
3. Use unified models for predictions on ANY symbol
4. Deploy to production

Models saved in: models/trained/

================================================================================
FINAL STATISTICS
================================================================================
  Approach:             Unified Dataset (All Symbols)
  Total Models:         3
  Successful:           3
  Failed:               0
  Success Rate:         100.0%
  Total Duration:       71.3s (1.2 min)
  Models per Symbol:    1 (unified model works for all)
================================================================================

ðŸŽ‰ Done! Your unified models are ready for deployment!