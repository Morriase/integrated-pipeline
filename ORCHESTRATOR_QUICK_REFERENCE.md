# Train All Models Orchestrator - Quick Reference

## Overview
The `train_all_models.py` orchestrator trains all SMC models with automatic model selection, comprehensive error handling, and enhanced reporting.

## Key Features

### ğŸ¯ Automatic Model Selection
- Filters models by quality criteria
- Selects best model per symbol
- Generates deployment manifest

### ğŸ›¡ï¸ Comprehensive Error Handling
- Continues training on individual failures
- Logs detailed error information
- Graceful degradation of components

### ğŸ“Š Enhanced Reporting
- Inline warnings in performance tables
- Model selection results
- Aggregated warnings summary
- Complete JSON output

## Usage

```bash
# Run complete training pipeline
python train_all_models.py
```

## Model Selection Criteria

| Criterion | Threshold | Purpose |
|-----------|-----------|---------|
| Train-Val Gap | â‰¤ 20% | Prevent overfitting |
| Test Accuracy | â‰¥ 55% | Ensure minimum quality |
| Val-Test Diff | â‰¤ 5% | Ensure stability |

**Scoring Formula:** `test_accuracy - (train_val_gap * 0.5)`

## Output Files

### 1. training_results.json
Complete training results with selections and warnings:
```json
{
  "training_results": {...},
  "model_selections": {...},
  "warnings": [...],
  "timestamp": "..."
}
```

### 2. deployment_manifest.json
Model selection manifest for deployment:
```json
{
  "timestamp": "...",
  "selection_criteria": {...},
  "selections": {
    "EURUSD": {
      "selected_model": "RandomForest",
      "test_accuracy": 0.623,
      "train_val_gap": 0.12,
      "reason": "...",
      "alternatives": [...]
    }
  },
  "summary": {...}
}
```

### 3. overfitting_report.md
Detailed overfitting analysis with visualizations

### 4. overfitting_analysis.png
Visual charts of overfitting metrics

## Warning Types

### ğŸ”´ Training Failures
- **Cause:** Model training crashed
- **Action:** Check error logs and traceback
- **Impact:** Model unavailable for selection

### ğŸŸ  Overfitting Detected
- **Cause:** Train-val gap > 15%
- **Action:** Apply stronger regularization
- **Impact:** Model may not generalize well

### ğŸŸ¡ CV Instability
- **Cause:** High variance across CV folds
- **Action:** Collect more data or simplify model
- **Impact:** Unreliable performance estimates

### ğŸŸ¢ Low Test Accuracy
- **Cause:** Test accuracy < 55%
- **Action:** Improve features or model architecture
- **Impact:** Model below quality threshold

## Error Handling Flow

```
Train Symbol
â”œâ”€â”€ Train RandomForest
â”‚   â”œâ”€â”€ Success â†’ Store results + duration
â”‚   â””â”€â”€ Failure â†’ Log error, continue
â”œâ”€â”€ Train XGBoost
â”‚   â”œâ”€â”€ Success â†’ Store results + duration
â”‚   â””â”€â”€ Failure â†’ Log error, continue
â”œâ”€â”€ Train NeuralNetwork
â”‚   â”œâ”€â”€ Success â†’ Store results + duration
â”‚   â””â”€â”€ Failure â†’ Log error, continue
â””â”€â”€ Train LSTM
    â”œâ”€â”€ Success â†’ Store results + duration
    â””â”€â”€ Failure â†’ Log error, continue

Select Best Models
â”œâ”€â”€ Success â†’ Generate manifest
â””â”€â”€ Failure â†’ Log error, continue

Generate Reports
â”œâ”€â”€ Overfitting Report
â”‚   â”œâ”€â”€ Success â†’ Save report
â”‚   â””â”€â”€ Failure â†’ Log warning, continue
â””â”€â”€ Summary Report
    â”œâ”€â”€ Success â†’ Save results
    â””â”€â”€ Failure â†’ Log error
```

## Model Selection Logic

### Step 1: Filter Models
```python
# Reject if:
- train_val_gap > 0.20  # Too much overfitting
- test_accuracy < 0.55  # Too low accuracy
- val_test_diff > 0.05  # Unstable performance
```

### Step 2: Score Candidates
```python
score = test_accuracy - (train_val_gap * 0.5)
```

### Step 3: Select Best
```python
selected_model = max(candidates, key=lambda x: x['score'])
```

### Step 4: Handle No Candidates
```python
if no_candidates:
    flag_for_manual_review()
```

## Summary Report Structure

```
TRAINING SUMMARY REPORT
â”œâ”€â”€ Per-Symbol Performance
â”‚   â”œâ”€â”€ Model name
â”‚   â”œâ”€â”€ CV MeanÂ±Std (if available)
â”‚   â”œâ”€â”€ Val/Test accuracy
â”‚   â”œâ”€â”€ Stability status
â”‚   â””â”€â”€ Warnings (inline)
â”‚
â”œâ”€â”€ MODEL SELECTION RESULTS
â”‚   â”œâ”€â”€ Selected model per symbol
â”‚   â”œâ”€â”€ Test accuracy & gap
â”‚   â”œâ”€â”€ Selection reason
â”‚   â””â”€â”€ Alternative models
â”‚
â”œâ”€â”€ WARNINGS SUMMARY
â”‚   â”œâ”€â”€ Total warning count
â”‚   â””â”€â”€ Detailed warning list
â”‚
â””â”€â”€ Final Statistics
    â”œâ”€â”€ Total symbols/models
    â”œâ”€â”€ Success/failure counts
    â”œâ”€â”€ Models selected
    â””â”€â”€ Training duration
```

## Interpreting Results

### âœ… Healthy Model
```
Model: RandomForest
CV: 0.6234Â±0.0123
Val: 0.625  Test: 0.623
Stability: âœ… Stable
Warnings: None
```

### âš ï¸ Problematic Model
```
Model: NeuralNetwork
CV: 0.7123Â±0.1567
Val: 0.712  Test: 0.598
Stability: âš ï¸ Unstable
Warnings: Overfitting (gap=23%); Unstable CV (std=0.157)
```

### âŒ Failed Model
```
Model: LSTM
Status: ERROR
Warnings: Training failed: RuntimeError
```

## Customizing Selection Criteria

Edit the main execution block:

```python
selector = ModelSelector(
    max_gap=0.15,           # Stricter: 15% max gap
    min_accuracy=0.60,      # Higher: 60% min accuracy
    stability_threshold=0.03  # Tighter: 3% max diff
)
```

## Troubleshooting

### No Models Selected for Symbol
**Symptom:** `selected_model: None` in manifest

**Causes:**
1. All models have high train-val gap (> 20%)
2. All models have low test accuracy (< 55%)
3. All models show val-test inconsistency (> 5%)

**Solutions:**
1. Review overfitting report
2. Apply anti-overfitting techniques
3. Collect more training data
4. Adjust selection criteria if too strict

### High Failure Rate
**Symptom:** Many models show ERROR status

**Causes:**
1. Missing dependencies (XGBoost, PyTorch)
2. Insufficient training data
3. Data quality issues
4. Memory constraints

**Solutions:**
1. Check error logs for specific issues
2. Verify data availability and quality
3. Install missing dependencies
4. Reduce model complexity or batch size

### Inconsistent Performance
**Symptom:** High CV std, unstable models

**Causes:**
1. Small dataset size
2. Imbalanced classes
3. High feature noise
4. Overly complex models

**Solutions:**
1. Collect more data
2. Apply data augmentation
3. Feature selection/engineering
4. Simplify model architecture

## Best Practices

1. **Review Warnings:** Always check warnings summary
2. **Validate Selections:** Review deployment manifest before production
3. **Monitor Trends:** Track overfitting metrics over time
4. **Iterate Criteria:** Adjust thresholds based on results
5. **Document Decisions:** Keep notes on manual reviews

## Integration with Pipeline

```bash
# Complete workflow
python run_complete_pipeline.py  # Prepare data
python train_all_models.py       # Train & select models
python ensemble_model.py          # Create ensemble (if needed)
python backtest.py                # Validate on test set
```

## Quick Checks

```bash
# Check if training completed
ls models/trained/training_results.json

# Check selected models
cat models/trained/deployment_manifest.json | grep selected_model

# Count warnings
cat models/trained/training_results.json | grep -c "warning"

# View overfitting summary
head -n 20 models/trained/overfitting_report.md
```

## Status Indicators

| Symbol | Meaning |
|--------|---------|
| âœ… | Healthy, no issues |
| âš ï¸ | Warning, review recommended |
| âŒ | Error, action required |
| ğŸ¯ | Selected for deployment |
| ğŸ”„ | Manual review needed |

## Next Steps After Training

1. âœ… Review `training_results.json`
2. âœ… Check `deployment_manifest.json`
3. âœ… Read `overfitting_report.md`
4. âš ï¸ Address flagged warnings
5. ğŸ¯ Deploy selected models
6. ğŸ“Š Monitor production performance
