================================================================================
SMC MODEL TRAINING - FULL DATASET APPROACH
================================================================================

🎯 Training Strategy:
  - Train on ALL symbols combined (unified dataset)
  - 2,500+ samples (vs 178-295 per symbol)
  - 4 models total (vs 44 models)
  - Better generalization, less overfitting
  - ATR normalization makes features comparable

⚠️  LSTM ENABLED (Experimental):
   - May show overfitting and gradient issues
   - Included for research/comparison purposes
   - Not recommended for production use
🔍 Detected Kaggle environment
📂 Data Directory: /kaggle/working/Data-output
📂 Model Output Directory: /kaggle/working/Model-output

🔍 Checking data availability...
  ✓ Train data: /kaggle/working/Data-output/processed_smc_data_train.csv
  ✓ Val data:   /kaggle/working/Data-output/processed_smc_data_val.csv
  ✓ Test data:  /kaggle/working/Data-output/processed_smc_data_test.csv

📊 Dataset Statistics:
  Train samples: 76,982
  Val samples:   16,500
  Test samples:  16,518
  Total:         110,000
  Symbols:       11
  Features:      82

================================================================================
STARTING UNIFIED TRAINING
================================================================================

################################################################################
# Training All Models on UNIFIED DATASET
################################################################################

🎯 Strategy: Train on ALL data for maximum generalization
   Core Models: RandomForest, XGBoost, NeuralNetwork
   Experimental: LSTM (may be unstable)

🌲 Starting RandomForest training...

================================================================================
Training Random Forest on FULL UNIFIED DATASET
================================================================================

📊 Training on FULL dataset:
  Train: 76,982 samples (11 symbols)
  Val:   16,500 samples
  Test:  16,518 samples
  Selected 57 features
  ⚠️ Removing 74,352 samples with NaN labels (96.6%)
  ⚠️ Removing 15,915 samples with NaN labels (96.5%)
  ⚠️ Removing 15,942 samples with NaN labels (96.5%)

🔄 Performing 5-fold cross-validation...

🔄 Performing 5-fold cross-validation...

🌲 Training Random Forest for UNIFIED...
  Training samples: 2,104
  Features: 57

  Train accuracy: 0.648
  Val accuracy:   0.652
  Train-Val gap:  -0.004
  ✅ Good generalization (gap ≤ 15%)

  Feature importance computed (feature names not available)
  Fold 1/5: Accuracy=0.652, F1=0.708

🌲 Training Random Forest for UNIFIED...
  Training samples: 2,104
  Features: 57

  Train accuracy: 0.661
  Val accuracy:   0.663
  Train-Val gap:  -0.003
  ✅ Good generalization (gap ≤ 15%)

  Feature importance computed (feature names not available)
  Fold 2/5: Accuracy=0.663, F1=0.743

🌲 Training Random Forest for UNIFIED...
  Training samples: 2,104
  Features: 57

  Train accuracy: 0.662
  Val accuracy:   0.660
  Train-Val gap:  0.002
  ✅ Good generalization (gap ≤ 15%)

  Feature importance computed (feature names not available)
  Fold 3/5: Accuracy=0.660, F1=0.747

🌲 Training Random Forest for UNIFIED...
  Training samples: 2,104
  Features: 57

  Train accuracy: 0.655
  Val accuracy:   0.650
  Train-Val gap:  0.005
  ✅ Good generalization (gap ≤ 15%)

  Feature importance computed (feature names not available)
  Fold 4/5: Accuracy=0.650, F1=0.747

🌲 Training Random Forest for UNIFIED...
  Training samples: 2,104
  Features: 57

  Train accuracy: 0.654
  Val accuracy:   0.641
  Train-Val gap:  0.014
  ✅ Good generalization (gap ≤ 15%)

  Feature importance computed (feature names not available)
  Fold 5/5: Accuracy=0.641, F1=0.714

  Cross-Validation Results:
    Mean Accuracy: 0.653 ± 0.008
    Min Accuracy:  0.641
    Max Accuracy:  0.663
    Range:         0.023
    Mean F1-Score: 0.732 ± 0.017
    ✅ STABLE: Std dev 0.008 ≤ 0.10

    Poor-Performing Folds (below mean - std):
      Fold 5: 0.641 (deviation: -0.013)

📈 Cross-Validation Results:
  Mean Accuracy: 0.6532 ± 0.0079
  Stability: ✅ Stable

🎯 Training final model...

🌲 Training Random Forest for UNIFIED...
  Training samples: 2,630
  Features: 57

  🔄 Performing cross-validation...

🔄 Performing 5-fold cross-validation...

🌲 Training Random Forest for UNIFIED...
  Training samples: 2,104
  Features: 57

  Train accuracy: 0.648
  Val accuracy:   0.652
  Train-Val gap:  -0.004
  ✅ Good generalization (gap ≤ 15%)

  Feature importance computed (feature names not available)
  Fold 1/5: Accuracy=0.652, F1=0.708

🌲 Training Random Forest for UNIFIED...
  Training samples: 2,104
  Features: 57

  Train accuracy: 0.661
  Val accuracy:   0.663
  Train-Val gap:  -0.003
  ✅ Good generalization (gap ≤ 15%)

  Feature importance computed (feature names not available)
  Fold 2/5: Accuracy=0.663, F1=0.743

🌲 Training Random Forest for UNIFIED...
  Training samples: 2,104
  Features: 57

  Train accuracy: 0.662
  Val accuracy:   0.660
  Train-Val gap:  0.002
  ✅ Good generalization (gap ≤ 15%)

  Feature importance computed (feature names not available)
  Fold 3/5: Accuracy=0.660, F1=0.747

🌲 Training Random Forest for UNIFIED...
  Training samples: 2,104
  Features: 57

  Train accuracy: 0.655
  Val accuracy:   0.650
  Train-Val gap:  0.005
  ✅ Good generalization (gap ≤ 15%)

  Feature importance computed (feature names not available)
  Fold 4/5: Accuracy=0.650, F1=0.747

🌲 Training Random Forest for UNIFIED...
  Training samples: 2,104
  Features: 57

  Train accuracy: 0.654
  Val accuracy:   0.641
  Train-Val gap:  0.014
  ✅ Good generalization (gap ≤ 15%)

  Feature importance computed (feature names not available)
  Fold 5/5: Accuracy=0.641, F1=0.714

  Cross-Validation Results:
    Mean Accuracy: 0.653 ± 0.008
    Min Accuracy:  0.641
    Max Accuracy:  0.663
    Range:         0.023
    Mean F1-Score: 0.732 ± 0.017
    ✅ STABLE: Std dev 0.008 ≤ 0.10

    Poor-Performing Folds (below mean - std):
      Fold 5: 0.641 (deviation: -0.013)

  Train accuracy: 0.657
  Val accuracy:   0.641
  Train-Val gap:  0.016
  ✅ Good generalization (gap ≤ 15%)

  Top 10 Most Important Features:
    TBM_Bars_to_Hit                         : 0.3556
    Distance_to_Entry_ATR                   : 0.1159
    TBM_Risk_Per_Trade_ATR                  : 0.1014
    TBM_Reward_Per_Trade_ATR                : 0.0977
    OB_Age                                  : 0.0562
    FVG_Depth_ATR                           : 0.0380
    FVG_Quality_Fuzzy                       : 0.0351
    FVG_Depth_ATR_ZScore                    : 0.0315
    FVG_Size_Fuzzy_Score                    : 0.0303
    FVG_Distance_to_Price_ATR               : 0.0249

📊 Evaluating on Validation set...

  Classification Metrics:
    Accuracy: 0.641
    Precision (macro): 0.717
    Recall (macro): 0.737
    F1-Score (macro): 0.724

  Trading Metrics (1:2.0 R:R):
    Win Rate: 49.6% (267/538 trades)
    Profit Factor: 1.97
    Expected Value/Trade: 0.49R
    Trade Accuracy: 61.8%
    Timeouts: 47 (8.0%)
    💰 PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Timeout  Pred Win
  True Loss         150           1        84
  True Timeout        0          43         1
  True Win          121           3       182

  Overfitting Analysis:
    Training Accuracy: 0.657
    Validation Accuracy: 0.641
    Train-Val Gap: 0.016
    ✅ No significant overfitting

📊 Evaluating on Test set...

  Classification Metrics:
    Accuracy: 0.615
    Precision (macro): 0.719
    Recall (macro): 0.720
    F1-Score (macro): 0.718

  Trading Metrics (1:2.0 R:R):
    Win Rate: 50.6% (278/549 trades)
    Profit Factor: 2.05
    Expected Value/Trade: 0.52R
    Trade Accuracy: 59.9%
    Timeouts: 27 (4.7%)
    💰 PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Timeout  Pred Win
  True Loss         146           1        95
  True Timeout        0          26         1
  True Win          125           0       182

💾 Model pickle saved to /kaggle/working/Model-output/UNIFIED_RandomForest.pkl
💾 Metadata saved to /kaggle/working/Model-output/UNIFIED_RandomForest_metadata.json

✅ Model save complete: /kaggle/working/Model-output
✅ RandomForest completed in 10.8s

🚀 Starting XGBoost training...

================================================================================
Training XGBoost on FULL UNIFIED DATASET
================================================================================

📊 Training on FULL dataset:
  Train: 76,982 samples (11 symbols)
  Val:   16,500 samples
  Test:  16,518 samples
  Selected 57 features
  ⚠️ Removing 74,352 samples with NaN labels (96.6%)
  ⚠️ Removing 15,915 samples with NaN labels (96.5%)
  ⚠️ Removing 15,942 samples with NaN labels (96.5%)

🎯 Training with aggressive regularization...

🚀 Training XGBoost for UNIFIED...
  Training samples: 2,630
  Features: 57

  Train accuracy: 0.775
  Val accuracy:   0.738
  Best iteration: 199

  Top 10 Most Important Features:
    OB_Age                                  : 0.1523
    TBM_Bars_to_Hit                         : 0.1331
    FVG_Mitigated                           : 0.0618
    OB_Mitigated                            : 0.0506
    FVG_Distance_to_Price_ATR               : 0.0439
    FVG_Depth_ATR                           : 0.0405
    TBM_Reward_Per_Trade_ATR                : 0.0367
    TBM_Risk_Per_Trade_ATR                  : 0.0365
    Distance_to_Entry_ATR                   : 0.0340
    FVG_Depth_ATR_ZScore                    : 0.0281

📊 Evaluating on Validation set...

  Classification Metrics:
    Accuracy: 0.738
    Precision (macro): 0.818
    Recall (macro): 0.819
    F1-Score (macro): 0.811

  Trading Metrics (1:2.0 R:R):
    Win Rate: 44.5% (241/541 trades)
    Profit Factor: 1.61
    Expected Value/Trade: 0.34R
    Trade Accuracy: 71.7%
    Timeouts: 44 (7.5%)
    💰 PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Timeout  Pred Win
  True Loss         191           0        44
  True Timeout        0          44         0
  True Win          109           0       197

  Overfitting Analysis:
    Training Accuracy: 0.775
    Validation Accuracy: 0.738
    Train-Val Gap: 0.036
    ✅ No significant overfitting

📊 Evaluating on Test set...

  Classification Metrics:
    Accuracy: 0.722
    Precision (macro): 0.814
    Recall (macro): 0.813
    F1-Score (macro): 0.806

  Trading Metrics (1:2.0 R:R):
    Win Rate: 42.8% (235/549 trades)
    Profit Factor: 1.50
    Expected Value/Trade: 0.28R
    Trade Accuracy: 70.9%
    Timeouts: 27 (4.7%)
    💰 PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Timeout  Pred Win
  True Loss         198           0        44
  True Timeout        0          27         0
  True Win          116           0       191

💾 Model pickle saved to /kaggle/working/Model-output/UNIFIED_XGBoost.pkl
💾 Metadata saved to /kaggle/working/Model-output/UNIFIED_XGBoost_metadata.json

✅ Model save complete: /kaggle/working/Model-output
✅ XGBoost completed in 2.4s

🧠 Starting Neural Network training...

================================================================================
Training Neural Network on FULL UNIFIED DATASET
================================================================================

📊 Training on FULL dataset:
  Train: 76,982 samples (11 symbols)
  Val:   16,500 samples
  Test:  16,518 samples
  Selected 57 features
  ⚠️ Removing 74,352 samples with NaN labels (96.6%)
  ⚠️ Removing 15,915 samples with NaN labels (96.5%)
  ⚠️ Removing 15,942 samples with NaN labels (96.5%)

🎯 Training with simplified architecture...

🧠 Training Neural Network for UNIFIED...
  Device: cuda
  Training samples: 2,630
  Features: 57
  Architecture: 57 -> 128 -> 64 -> 32 -> 3
⚠️ Epoch 1: Exploding gradient detected (norm=10.46 > 10.0)
  Epoch 10/200 - LR: 0.001000 - Train Loss: 1.0933, Train Acc: 0.440 | Val Loss: 1.0155, Val Acc: 0.653
  Epoch 20/200 - LR: 0.001000 - Train Loss: 1.0616, Train Acc: 0.556 | Val Loss: 1.0123, Val Acc: 0.648
  Epoch 30/200 - LR: 0.001000 - Train Loss: 1.0444, Train Acc: 0.641 | Val Loss: 1.0073, Val Acc: 0.672
  Epoch 40/200 - LR: 0.001000 - Train Loss: 1.0182, Train Acc: 0.683 | Val Loss: 1.0019, Val Acc: 0.677
  Epoch 50/200 - LR: 0.001000 - Train Loss: 1.0114, Train Acc: 0.687 | Val Loss: 0.9967, Val Acc: 0.679
  Epoch 60/200 - LR: 0.001000 - Train Loss: 1.0016, Train Acc: 0.693 | Val Loss: 0.9946, Val Acc: 0.682
  Epoch 70/200 - LR: 0.000500 - Train Loss: 1.0019, Train Acc: 0.701 | Val Loss: 0.9937, Val Acc: 0.679
  Epoch 80/200 - LR: 0.000500 - Train Loss: 1.0151, Train Acc: 0.712 | Val Loss: 0.9955, Val Acc: 0.680
  Epoch 90/200 - LR: 0.000125 - Train Loss: 1.0146, Train Acc: 0.698 | Val Loss: 0.9955, Val Acc: 0.687
  Epoch 100/200 - LR: 0.000063 - Train Loss: 1.0176, Train Acc: 0.698 | Val Loss: 0.9938, Val Acc: 0.691
  Epoch 110/200 - LR: 0.000031 - Train Loss: 1.0095, Train Acc: 0.714 | Val Loss: 0.9954, Val Acc: 0.687
  Epoch 120/200 - LR: 0.000008 - Train Loss: 0.9884, Train Acc: 0.721 | Val Loss: 0.9908, Val Acc: 0.691

  Early stopping at epoch 121 (patience=20)
Learning curves saved to: models/trained/UNIFIED_NN_learning_curves.png
Overfitting metrics saved to: models/trained/UNIFIED_NN_overfitting_metrics.json

============================================================
OVERFITTING MONITOR SUMMARY
============================================================
Status: ✓ HEALTHY
Train-Val Gap: 2.61% (Threshold: 15%)
Total Epochs: 121
Training Duration: 0:00:22.715321

Final Metrics:
  Train Accuracy: 0.7133
  Val Accuracy:   0.6872
  Train Loss:     1.0091
  Val Loss:       0.9926

Best Epoch (Highest Val Accuracy):
  Epoch:          105
  Train Accuracy: 0.7023
  Val Accuracy:   0.6957
  Train Loss:     1.0097
  Val Loss:       0.9924
============================================================


  ⚠️ Training Warnings (1):
    - Epoch 1: Exploding gradient detected (norm=10.46 > 10.0)

  Final Train Accuracy: 0.713
  Final Val Accuracy:   0.687
  Train-Val Gap:        0.026 (2.6%)

📊 Evaluating on Validation set...

  Classification Metrics:
    Accuracy: 0.687
    Precision (macro): 0.770
    Recall (macro): 0.769
    F1-Score (macro): 0.769

  Trading Metrics (1:2.0 R:R):
    Win Rate: 58.2% (315/541 trades)
    Profit Factor: 2.79
    Expected Value/Trade: 0.75R
    Trade Accuracy: 66.2%
    Timeouts: 44 (7.5%)
    💰 PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Timeout  Pred Win
  True Loss         139           0        96
  True Timeout        0          44         0
  True Win           87           0       219

📊 Evaluating on Test set...

  Classification Metrics:
    Accuracy: 0.648
    Precision (macro): 0.750
    Recall (macro): 0.749
    F1-Score (macro): 0.749

  Trading Metrics (1:2.0 R:R):
    Win Rate: 57.2% (314/549 trades)
    Profit Factor: 2.67
    Expected Value/Trade: 0.72R
    Trade Accuracy: 63.0%
    Timeouts: 27 (4.7%)
    💰 PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Timeout  Pred Win
  True Loss         137           0       105
  True Timeout        0          27         0
  True Win           98           0       209

💾 Model pickle saved to /kaggle/working/Model-output/UNIFIED_NeuralNetwork.pkl
💾 Metadata saved to /kaggle/working/Model-output/UNIFIED_NeuralNetwork_metadata.json
💾 Scaler saved to /kaggle/working/Model-output/UNIFIED_NeuralNetwork_scaler.pkl

✅ Model save complete: /kaggle/working/Model-output
✅ Neural Network completed in 28.6s

🔄 Starting LSTM training (EXPERIMENTAL)...

================================================================================
Training LSTM on FULL UNIFIED DATASET (EXPERIMENTAL)
================================================================================

⚠️  WARNING: LSTM has shown instability in previous tests:
   - Severe overfitting (39-61% train-val gap)
   - Gradient explosions (24 warnings)
   - Poor test accuracy (16-46%)
   - Training divergence

   This model is included for experimental purposes only.

📊 Training on FULL dataset:
  Train: 76,982 samples (11 symbols)
  Val:   16,500 samples
  Test:  16,518 samples
  Lookback: 10 candles
  Selected 57 features
  ⚠️ Removing 74,352 samples with NaN labels (96.6%)
  ⚠️ Removing 15,915 samples with NaN labels (96.5%)
  ⚠️ Removing 15,942 samples with NaN labels (96.5%)

🎯 Training with AGGRESSIVE regularization...

🔄 Training LSTM for UNIFIED...
  Device: cuda
  Lookback window: 10 candles
  Training samples: 2,630
  Features: 57
  Creating sequences...
  Sequence shape: (2620, 10, 57)
⚠️ Epoch 1: Exploding gradient detected (norm=5.62 > 5.0)
⚠️ Epoch 1: Exploding gradient detected (norm=7.39 > 5.0)
⚠️ Epoch 2: Exploding gradient detected (norm=5.73 > 5.0)
⚠️ Epoch 2: Exploding gradient detected (norm=5.01 > 5.0)
⚠️ Epoch 2: Exploding gradient detected (norm=5.52 > 5.0)
⚠️ Epoch 2: Exploding gradient detected (norm=6.94 > 5.0)
⚠️ Epoch 2: Exploding gradient detected (norm=5.61 > 5.0)
⚠️ Epoch 3: Exploding gradient detected (norm=5.99 > 5.0)
⚠️ Epoch 4: Exploding gradient detected (norm=8.18 > 5.0)
⚠️ Epoch 4: Exploding gradient detected (norm=8.00 > 5.0)
⚠️ Epoch 5: Exploding gradient detected (norm=11.69 > 5.0)
⚠️ Epoch 5: Exploding gradient detected (norm=6.29 > 5.0)
⚠️ Epoch 5: Exploding gradient detected (norm=7.34 > 5.0)
⚠️ Epoch 6: Exploding gradient detected (norm=6.07 > 5.0)
⚠️ Epoch 7: Exploding gradient detected (norm=5.05 > 5.0)
⚠️ Epoch 7: Exploding gradient detected (norm=6.53 > 5.0)
⚠️ Epoch 7: Exploding gradient detected (norm=5.38 > 5.0)
⚠️ Epoch 8: Exploding gradient detected (norm=6.24 > 5.0)
⚠️ Epoch 8: Exploding gradient detected (norm=5.09 > 5.0)
⚠️ Epoch 8: Exploding gradient detected (norm=5.07 > 5.0)
⚠️ Epoch 8: Exploding gradient detected (norm=7.10 > 5.0)
⚠️ Epoch 9: Exploding gradient detected (norm=5.80 > 5.0)
⚠️ Epoch 9: Exploding gradient detected (norm=5.82 > 5.0)
⚠️ Epoch 10: Exploding gradient detected (norm=5.12 > 5.0)
⚠️ Epoch 10: Exploding gradient detected (norm=7.60 > 5.0)
  Epoch 10/200 - LR: 0.000010 - Train Loss: 1.1442, Train Acc: 0.417 | Val Loss: 1.1026, Val Acc: 0.386
⚠️ Epoch 11: Exploding gradient detected (norm=6.47 > 5.0)
⚠️ Epoch 11: Exploding gradient detected (norm=6.16 > 5.0)
⚠️ Epoch 11: Exploding gradient detected (norm=8.95 > 5.0)
⚠️ Epoch 12: Exploding gradient detected (norm=6.08 > 5.0)
⚠️ Epoch 12: Exploding gradient detected (norm=6.78 > 5.0)
⚠️ Epoch 13: Exploding gradient detected (norm=5.98 > 5.0)
⚠️ Epoch 14: Exploding gradient detected (norm=6.17 > 5.0)
⚠️ Epoch 14: Exploding gradient detected (norm=9.32 > 5.0)
⚠️ Epoch 15: Exploding gradient detected (norm=5.23 > 5.0)
⚠️ Epoch 15: Exploding gradient detected (norm=6.29 > 5.0)
⚠️ Epoch 16: Exploding gradient detected (norm=8.73 > 5.0)
⚠️ Epoch 16: Exploding gradient detected (norm=5.44 > 5.0)
⚠️ Epoch 17: Exploding gradient detected (norm=5.96 > 5.0)
⚠️ Epoch 17: Exploding gradient detected (norm=6.08 > 5.0)
⚠️ Epoch 18: Exploding gradient detected (norm=5.63 > 5.0)
⚠️ Epoch 18: Exploding gradient detected (norm=5.33 > 5.0)
⚠️ Epoch 18: Exploding gradient detected (norm=5.48 > 5.0)
⚠️ Epoch 18: Exploding gradient detected (norm=11.41 > 5.0)
⚠️ Epoch 19: Exploding gradient detected (norm=5.97 > 5.0)
⚠️ Epoch 19: Exploding gradient detected (norm=5.36 > 5.0)
⚠️ Epoch 19: Exploding gradient detected (norm=6.25 > 5.0)
⚠️ Epoch 19: Exploding gradient detected (norm=7.10 > 5.0)
⚠️ Epoch 20: Exploding gradient detected (norm=6.21 > 5.0)
⚠️ Epoch 20: Exploding gradient detected (norm=5.65 > 5.0)
  Epoch 20/200 - LR: 0.000028 - Train Loss: 1.1331, Train Acc: 0.423 | Val Loss: 1.0992, Val Acc: 0.400
⚠️ Epoch 21: Exploding gradient detected (norm=12.64 > 5.0)
⚠️ Epoch 22: Exploding gradient detected (norm=5.10 > 5.0)
⚠️ Epoch 24: Exploding gradient detected (norm=9.69 > 5.0)
⚠️ Epoch 25: Exploding gradient detected (norm=7.22 > 5.0)
⚠️ Epoch 26: Exploding gradient detected (norm=5.35 > 5.0)
⚠️ Epoch 27: Exploding gradient detected (norm=5.75 > 5.0)
⚠️ Epoch 29: Exploding gradient detected (norm=6.69 > 5.0)
⚠️ Epoch 30: Exploding gradient detected (norm=5.27 > 5.0)
  Epoch 30/200 - LR: 0.000052 - Train Loss: 1.1039, Train Acc: 0.458 | Val Loss: 1.0993, Val Acc: 0.424
⚠️ Epoch 34: Exploding gradient detected (norm=8.37 > 5.0)
⚠️ Epoch 34: Exploding gradient detected (norm=9.38 > 5.0)
⚠️ Epoch 35: Exploding gradient detected (norm=5.73 > 5.0)
⚠️ Epoch 36: Exploding gradient detected (norm=6.17 > 5.0)
⚠️ Epoch 37: Exploding gradient detected (norm=5.09 > 5.0)
⚠️ Epoch 38: Exploding gradient detected (norm=5.59 > 5.0)

  Early stopping at epoch 38

  ⚠️ Training Warnings (63):
    - Epoch 1: Exploding gradient detected (norm=5.62 > 5.0)
    - Epoch 1: Exploding gradient detected (norm=7.39 > 5.0)
    - Epoch 2: Exploding gradient detected (norm=5.73 > 5.0)
    - Epoch 2: Exploding gradient detected (norm=5.01 > 5.0)
    - Epoch 2: Exploding gradient detected (norm=5.52 > 5.0)
    - Epoch 2: Exploding gradient detected (norm=6.94 > 5.0)
    - Epoch 2: Exploding gradient detected (norm=5.61 > 5.0)
    - Epoch 3: Exploding gradient detected (norm=5.99 > 5.0)
    - Epoch 4: Exploding gradient detected (norm=8.18 > 5.0)
    - Epoch 4: Exploding gradient detected (norm=8.00 > 5.0)
    - Epoch 5: Exploding gradient detected (norm=11.69 > 5.0)
    - Epoch 5: Exploding gradient detected (norm=6.29 > 5.0)
    - Epoch 5: Exploding gradient detected (norm=7.34 > 5.0)
    - Epoch 6: Exploding gradient detected (norm=6.07 > 5.0)
    - Epoch 7: Exploding gradient detected (norm=5.05 > 5.0)
    - Epoch 7: Exploding gradient detected (norm=6.53 > 5.0)
    - Epoch 7: Exploding gradient detected (norm=5.38 > 5.0)
    - Epoch 8: Exploding gradient detected (norm=6.24 > 5.0)
    - Epoch 8: Exploding gradient detected (norm=5.09 > 5.0)
    - Epoch 8: Exploding gradient detected (norm=5.07 > 5.0)
    - Epoch 8: Exploding gradient detected (norm=7.10 > 5.0)
    - Epoch 9: Exploding gradient detected (norm=5.80 > 5.0)
    - Epoch 9: Exploding gradient detected (norm=5.82 > 5.0)
    - Epoch 10: Exploding gradient detected (norm=5.12 > 5.0)
    - Epoch 10: Exploding gradient detected (norm=7.60 > 5.0)
    - Epoch 11: Exploding gradient detected (norm=6.47 > 5.0)
    - Epoch 11: Exploding gradient detected (norm=6.16 > 5.0)
    - Epoch 11: Exploding gradient detected (norm=8.95 > 5.0)
    - Epoch 12: Exploding gradient detected (norm=6.08 > 5.0)
    - Epoch 12: Exploding gradient detected (norm=6.78 > 5.0)
    - Epoch 13: Exploding gradient detected (norm=5.98 > 5.0)
    - Epoch 14: Exploding gradient detected (norm=6.17 > 5.0)
    - Epoch 14: Exploding gradient detected (norm=9.32 > 5.0)
    - Epoch 15: Exploding gradient detected (norm=5.23 > 5.0)
    - Epoch 15: Exploding gradient detected (norm=6.29 > 5.0)
    - Epoch 16: Exploding gradient detected (norm=8.73 > 5.0)
    - Epoch 16: Exploding gradient detected (norm=5.44 > 5.0)
    - Epoch 17: Exploding gradient detected (norm=5.96 > 5.0)
    - Epoch 17: Exploding gradient detected (norm=6.08 > 5.0)
    - Epoch 18: Exploding gradient detected (norm=5.63 > 5.0)
    - Epoch 18: Exploding gradient detected (norm=5.33 > 5.0)
    - Epoch 18: Exploding gradient detected (norm=5.48 > 5.0)
    - Epoch 18: Exploding gradient detected (norm=11.41 > 5.0)
    - Epoch 19: Exploding gradient detected (norm=5.97 > 5.0)
    - Epoch 19: Exploding gradient detected (norm=5.36 > 5.0)
    - Epoch 19: Exploding gradient detected (norm=6.25 > 5.0)
    - Epoch 19: Exploding gradient detected (norm=7.10 > 5.0)
    - Epoch 20: Exploding gradient detected (norm=6.21 > 5.0)
    - Epoch 20: Exploding gradient detected (norm=5.65 > 5.0)
    - Epoch 21: Exploding gradient detected (norm=12.64 > 5.0)
    - Epoch 22: Exploding gradient detected (norm=5.10 > 5.0)
    - Epoch 24: Exploding gradient detected (norm=9.69 > 5.0)
    - Epoch 25: Exploding gradient detected (norm=7.22 > 5.0)
    - Epoch 26: Exploding gradient detected (norm=5.35 > 5.0)
    - Epoch 27: Exploding gradient detected (norm=5.75 > 5.0)
    - Epoch 29: Exploding gradient detected (norm=6.69 > 5.0)
    - Epoch 30: Exploding gradient detected (norm=5.27 > 5.0)
    - Epoch 34: Exploding gradient detected (norm=8.37 > 5.0)
    - Epoch 34: Exploding gradient detected (norm=9.38 > 5.0)
    - Epoch 35: Exploding gradient detected (norm=5.73 > 5.0)
    - Epoch 36: Exploding gradient detected (norm=6.17 > 5.0)
    - Epoch 37: Exploding gradient detected (norm=5.09 > 5.0)
    - Epoch 38: Exploding gradient detected (norm=5.59 > 5.0)

  Final Train Accuracy: 0.468
  Final Val Accuracy:   0.449
  Train-Val Gap:        0.019 (1.9%)

📊 Evaluating on Validation set...

  Classification Metrics:
    Accuracy: 0.443
    Precision (macro): 0.334
    Recall (macro): 0.329
    F1-Score (macro): 0.321

  Trading Metrics (1:2.0 R:R):
    Win Rate: 45.3% (260/574 trades)
    Profit Factor: 1.66
    Expected Value/Trade: 0.36R
    Trade Accuracy: 48.6%
    Timeouts: 11 (1.9%)
    💰 PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Timeout  Pred Win
  True Loss         124           3       108
  True Timeout       25           1        18
  True Win          165           7       134

📊 Evaluating on Test set...

  Classification Metrics:
    Accuracy: 0.464
    Precision (macro): 0.317
    Recall (macro): 0.326
    F1-Score (macro): 0.319

  Trading Metrics (1:2.0 R:R):
    Win Rate: 47.6% (269/565 trades)
    Profit Factor: 1.82
    Expected Value/Trade: 0.43R
    Trade Accuracy: 49.6%
    Timeouts: 11 (1.9%)
    💰 PROFITABLE STRATEGY (EV > 0)

  Confusion Matrix:
              Pred Loss  Pred Timeout  Pred Win
  True Loss         124           6       112
  True Timeout       13           0        14
  True Win          159           5       143

💾 Model pickle saved to /kaggle/working/Model-output/UNIFIED_LSTM.pkl
💾 Metadata saved to /kaggle/working/Model-output/UNIFIED_LSTM_metadata.json
💾 Scaler saved to /kaggle/working/Model-output/UNIFIED_LSTM_scaler.pkl

✅ Model save complete: /kaggle/working/Model-output
⚠️ LSTM completed in 27.1s but showed 63 warnings

================================================================================
Training Summary:
  Total Duration: 68.9s (1.1 min)
  Successful Models: 4/4
  Failed Models: 0/4
================================================================================

================================================================================
All training completed in 68.9s (1.1 minutes)
================================================================================

================================================================================
TRAINING SUMMARY REPORT - UNIFIED DATASET
================================================================================

📊 Model Performance:
  Model                Train Acc    Val Acc      Test Acc     Gap        Status              
  ----------------------------------------------------------------------------------------------------
  RandomForest         0.657        0.641        0.615        1.60%      ✅ Good              
  XGBoost              0.775        0.738        0.722        3.61%      ✅ Good              
  NeuralNetwork        0.000        0.687        0.648        -68.72%    ✅ Good              
  LSTM                 0.000        0.443        0.464        -44.27%    ⚠️ Unstable (63 warnings)

💾 Results saved to: /kaggle/working/Model-output/training_results.json

✅ Training complete!

================================================================================
NEXT STEPS:
================================================================================
1. Review training results in models/trained/training_results.json
2. Check model files: UNIFIED_RandomForest.pkl, UNIFIED_XGBoost.pkl, UNIFIED_NeuralNetwork.pkl
3. Use unified models for predictions on ANY symbol
4. Deploy to production

Models saved in: models/trained/

================================================================================
FINAL STATISTICS
================================================================================
  Approach:             Unified Dataset (All Symbols)
  Total Models:         4
  Successful:           4
  Failed:               0
  Success Rate:         100.0%
  Total Duration:       68.9s (1.1 min)
  Models per Symbol:    1 (unified model works for all)
  LSTM Status:          ✅ Trained
================================================================================

🎉 Done! Your unified models are ready for deployment!

⚠️  Note: LSTM showed 63 training warnings.
   Consider using RF, XGB, or NN for production instead.