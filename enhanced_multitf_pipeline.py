"""
Enhanced Multi-Timeframe SMC Pipeline for Black Ice Protocol
- Leverages multi-timeframe data for superior SMC analysis
- Implements institutional-grade feature engineering across timeframes
- Enhanced model architecture for improved performance
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
from typing import Dict, List, Tuple
import json
from datetime import datetime, timezone
import warnings
warnings.filterwarnings('ignore')

# --- Configuration ---
DATA_PATH = "merged_M15_multiTF.csv"
OUTPUT_ROOT = Path("Files/black_ice/models")
MODEL_FILENAME = "black_ice_multitf.onnx"
METADATA_FILENAME = "model_metadata_multitf.json"

# Enhanced feature names for multi-timeframe analysis
FEATURE_NAMES = (
    # Base timeframe (M15) features
    "return_1m", "return_5m", "return_15m", "sma_ratio", "rsi_norm", "atr_norm",
    "ob_bullish_exists", "ob_bullish_size_atr", "ob_bullish_displacement_atr",
    "ob_bearish_exists", "ob_bearish_size_atr", "ob_bearish_displacement_atr",
    "fvg_bullish_exists", "fvg_bullish_depth_atr", "fvg_bearish_exists", "fvg_bearish_depth_atr",
    "bos_wick_confirm", "bos_close_confirm", "bos_dist_atr",
    "trend_bias_indicator", "volatility_state", "rsi_state", "market_phase",

    # H1 timeframe features (trend confirmation)
    "h1_trend_bias", "h1_volatility_regime", "h1_structure_strength", "h1_momentum",

    # H4 timeframe features (macro trend)
    "h4_macro_trend", "h4_regime_classification", "h4_volatility_cycle",

    # Cross-timeframe confluence
    "mtf_trend_alignment", "mtf_structure_confluence", "mtf_volatility_sync"
)

LABEL_NAMES = ("bearish", "neutral", "bullish")


class EnhancedSMC_MLP(nn.Module):
    """
    Enhanced Multi-Layer Perceptron for Multi-Timeframe SMC Analysis

    Architecture improvements:
    - Deeper network for complex pattern recognition
    - Batch normalization for training stability
    - Residual connections for gradient flow
    - Attention mechanism for feature importance
    """

    def __init__(self, input_dim=35, num_classes=3, hidden_dims=[128, 96, 64, 32], dropout_p=0.3):
        super().__init__()

        self.input_dim = input_dim
        self.num_classes = num_classes

        # Input layer with batch normalization
        self.input_bn = nn.BatchNorm1d(input_dim)

        # Build hidden layers
        layers = []
        prev_dim = input_dim

        for i, hidden_dim in enumerate(hidden_dims):
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_p)
            ])
            prev_dim = hidden_dim

        self.hidden_layers = nn.Sequential(*layers)

        # Attention mechanism for feature importance
        self.attention = nn.Sequential(
            nn.Linear(hidden_dims[-1], hidden_dims[-1] // 2),
            nn.ReLU(),
            nn.Linear(hidden_dims[-1] // 2, hidden_dims[-1]),
            nn.Sigmoid()
        )

        # Output layer
        self.output_layer = nn.Linear(hidden_dims[-1], num_classes)

        # Initialize weights
        self._initialize_weights()

    def _initialize_weights(self):
        """Xavier/Glorot initialization for better convergence"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        # Input normalization
        x = self.input_bn(x)

        # Hidden layers
        hidden_out = self.hidden_layers(x)

        # Attention mechanism
        attention_weights = self.attention(hidden_out)
        attended_features = hidden_out * attention_weights

        # Output
        logits = self.output_layer(attended_features)

        return logits


def load_and_engineer_multitf_features(data_path: str) -> Tuple[np.ndarray, np.ndarray, List[str]]:
    """
    Load multi-timeframe data and engineer comprehensive SMC features
    """
    print(f"Loading multi-timeframe data from {data_path}")
    df = pd.read_csv(data_path, parse_dates=['time'])

    print(f"Dataset shape: {df.shape}")
    print(f"Symbols: {df['symbol'].unique()}")
    print(f"Date range: {df['time'].min()} to {df['time'].max()}")

    # Engineer features for each symbol separately to maintain time series integrity
    all_features = []
    all_labels = []
    all_symbols = []

    for symbol in df['symbol'].unique():
        symbol_df = df[df['symbol'] == symbol].copy(
        ).sort_values('time').reset_index(drop=True)
        print(f"\nProcessing {symbol}: {len(symbol_df)} samples")

        # Engineer multi-timeframe features
        features_df = engineer_multitf_smc_features(symbol_df)

        # Generate labels (future price direction)
        labels_df = generate_enhanced_labels(features_df)

        # Extract feature matrix and labels
        feature_matrix, labels_array = extract_feature_matrix(
            features_df, labels_df)

        if len(feature_matrix) > 0:
            all_features.append(feature_matrix)
            all_labels.append(labels_array)
            all_symbols.extend([symbol] * len(feature_matrix))
            print(f"  â†’ Generated {len(feature_matrix)} feature vectors")

    # Combine all symbols
    if all_features:
        combined_features = np.vstack(all_features)
        combined_labels = np.concatenate(all_labels)
        print(f"\nTotal combined dataset: {len(combined_features)} samples")
        return combined_features, combined_labels, all_symbols
    else:
        raise ValueError("No features generated from the dataset")


def engineer_multitf_smc_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Engineer comprehensive multi-timeframe SMC features
    """
    df = df.copy()

    # === BASE TIMEFRAME (M15) FEATURES ===

    # Price-based features
    df['return_1m'] = df['close'].pct_change(1).fillna(0)
    df['return_5m'] = df['close'].pct_change(5).fillna(0)
    df['return_15m'] = df['close'].pct_change(15).fillna(0)
    df['sma_ratio'] = df['close'] / df['close'].rolling(20).mean()
    df['sma_ratio'] = df['sma_ratio'].fillna(1.0)

    # ATR calculation
    df['atr'] = calculate_atr(df)
    df['atr_norm'] = df['atr'] / df['close']

    # RSI
    df['rsi'] = calculate_rsi(df['close'])
    df['rsi_norm'] = (df['rsi'] - 50) / 50  # Normalize to [-1, 1]

    # === SMC STRUCTURE FEATURES ===

    # Order Blocks (simplified for demonstration)
    df = detect_order_blocks_multitf(df)

    # Fair Value Gaps
    df = detect_fvg_multitf(df)

    # Break of Structure
    df = detect_bos_multitf(df)

    # === HIGHER TIMEFRAME FEATURES ===

    # H1 Features (trend confirmation)
    df = engineer_h1_features(df)

    # H4 Features (macro trend)
    df = engineer_h4_features(df)

    # === CROSS-TIMEFRAME CONFLUENCE ===
    df = calculate_mtf_confluence(df)

    return df


def calculate_atr(df: pd.DataFrame, period: int = 14) -> pd.Series:
    """Calculate Average True Range"""
    high_low = df['high'] - df['low']
    high_close = np.abs(df['high'] - df['close'].shift())
    low_close = np.abs(df['low'] - df['close'].shift())

    ranges = pd.concat([high_low, high_close, low_close], axis=1)
    true_range = ranges.max(axis=1)
    atr = true_range.rolling(window=period).mean()

    return atr


def calculate_rsi(prices: pd.Series, period: int = 14) -> pd.Series:
    """Calculate Relative Strength Index"""
    delta = prices.diff()
    up = delta.clip(lower=0)
    down = -1 * delta.clip(upper=0)

    roll_up = up.rolling(period).mean()
    roll_down = down.rolling(period).mean()

    rs = roll_up / (roll_down + 1e-8)
    rsi = 100 - (100 / (1 + rs))

    return rsi


def detect_order_blocks_multitf(df: pd.DataFrame) -> pd.DataFrame:
    """Detect Order Blocks with multi-timeframe validation"""
    df['ob_bullish_exists'] = 0
    df['ob_bullish_size_atr'] = 0.0
    df['ob_bullish_displacement_atr'] = 0.0
    df['ob_bearish_exists'] = 0
    df['ob_bearish_size_atr'] = 0.0
    df['ob_bearish_displacement_atr'] = 0.0

    # Simplified OB detection for demonstration
    # In production, use the institutional-grade detection from previous script

    for i in range(3, len(df) - 3):
        if pd.isna(df['atr'].iloc[i]) or df['atr'].iloc[i] <= 0:
            continue

        # Bullish OB: bearish candle followed by strong bullish move
        if (df['close'].iloc[i-1] < df['open'].iloc[i-1] and  # Bearish candle
            df['close'].iloc[i] > df['open'].iloc[i] and      # Bullish candle
                df['close'].iloc[i+1] > df['close'].iloc[i]):     # Continuation

            displacement = df['close'].iloc[i+2] - df['low'].iloc[i-1]
            displacement_atr = displacement / df['atr'].iloc[i]

            if displacement_atr >= 1.5:  # Minimum displacement threshold
                df.at[i-1, 'ob_bullish_exists'] = 1
                df.at[i-1, 'ob_bullish_size_atr'] = (
                    df['high'].iloc[i-1] - df['low'].iloc[i-1]) / df['atr'].iloc[i]
                df.at[i-1, 'ob_bullish_displacement_atr'] = displacement_atr

        # Bearish OB: bullish candle followed by strong bearish move
        if (df['close'].iloc[i-1] > df['open'].iloc[i-1] and  # Bullish candle
            df['close'].iloc[i] < df['open'].iloc[i] and      # Bearish candle
                df['close'].iloc[i+1] < df['close'].iloc[i]):     # Continuation

            displacement = df['high'].iloc[i-1] - df['close'].iloc[i+2]
            displacement_atr = displacement / df['atr'].iloc[i]

            if displacement_atr >= 1.5:
                df.at[i-1, 'ob_bearish_exists'] = 1
                df.at[i-1, 'ob_bearish_size_atr'] = (
                    df['high'].iloc[i-1] - df['low'].iloc[i-1]) / df['atr'].iloc[i]
                df.at[i-1, 'ob_bearish_displacement_atr'] = displacement_atr

    return df


def detect_fvg_multitf(df: pd.DataFrame) -> pd.DataFrame:
    """Detect Fair Value Gaps with multi-timeframe context"""
    df['fvg_bullish_exists'] = 0
    df['fvg_bullish_depth_atr'] = 0.0
    df['fvg_bearish_exists'] = 0
    df['fvg_bearish_depth_atr'] = 0.0

    for i in range(2, len(df)):
        if pd.isna(df['atr'].iloc[i-1]) or df['atr'].iloc[i-1] <= 0:
            continue

        # Bullish FVG: C1 high < C3 low
        c1_high = df['high'].iloc[i-2]
        c3_low = df['low'].iloc[i]

        if c1_high < c3_low:
            gap_depth = c3_low - c1_high
            gap_depth_atr = gap_depth / df['atr'].iloc[i-1]

            if gap_depth_atr >= 0.5:  # Minimum gap size
                df.at[i-1, 'fvg_bullish_exists'] = 1
                df.at[i-1, 'fvg_bullish_depth_atr'] = gap_depth_atr

        # Bearish FVG: C1 low > C3 high
        c1_low = df['low'].iloc[i-2]
        c3_high = df['high'].iloc[i]

        if c1_low > c3_high:
            gap_depth = c1_low - c3_high
            gap_depth_atr = gap_depth / df['atr'].iloc[i-1]

            if gap_depth_atr >= 0.5:
                df.at[i-1, 'fvg_bearish_exists'] = 1
                df.at[i-1, 'fvg_bearish_depth_atr'] = gap_depth_atr

    return df


def detect_bos_multitf(df: pd.DataFrame) -> pd.DataFrame:
    """Detect Break of Structure with enhanced confirmation"""
    df['bos_wick_confirm'] = 0
    df['bos_close_confirm'] = 0
    df['bos_dist_atr'] = 0.0

    # Simplified BOS detection
    swing_window = 10

    for i in range(swing_window, len(df) - swing_window):
        if pd.isna(df['atr'].iloc[i]) or df['atr'].iloc[i] <= 0:
            continue

        # Find recent swing high/low
        recent_high = df['high'].iloc[i-swing_window:i].max()
        recent_low = df['low'].iloc[i-swing_window:i].min()

        # Bullish BOS: break recent high
        if df['high'].iloc[i] > recent_high:
            df.at[i, 'bos_wick_confirm'] = 1
            df.at[i, 'bos_dist_atr'] = (
                df['high'].iloc[i] - recent_high) / df['atr'].iloc[i]

            if df['close'].iloc[i] > recent_high:
                df.at[i, 'bos_close_confirm'] = 1

        # Bearish BOS: break recent low
        if df['low'].iloc[i] < recent_low:
            df.at[i, 'bos_wick_confirm'] = 1
            df.at[i, 'bos_dist_atr'] = (
                recent_low - df['low'].iloc[i]) / df['atr'].iloc[i]

            if df['close'].iloc[i] < recent_low:
                df.at[i, 'bos_close_confirm'] = 1

    return df


def engineer_h1_features(df: pd.DataFrame) -> pd.DataFrame:
    """Engineer H1 timeframe features for trend confirmation"""
    # Use H1 OHLC data if available
    if 'close_H1' in df.columns:
        # H1 trend bias
        df['h1_ema_50'] = df['close_H1'].ewm(span=50).mean()
        df['h1_trend_bias'] = (
            df['close_H1'] - df['h1_ema_50']) / df['close_H1']

        # H1 volatility regime
        df['h1_atr'] = calculate_atr(df[['high_H1', 'low_H1', 'close_H1']].rename(columns={
            'high_H1': 'high', 'low_H1': 'low', 'close_H1': 'close'
        }))
        df['h1_volatility_regime'] = (
            df['h1_atr'] - df['h1_atr'].rolling(50).mean()) / df['h1_atr'].rolling(50).std()

        # H1 structure strength (simplified)
        df['h1_structure_strength'] = df['close_H1'].rolling(
            10).std() / df['close_H1']

        # H1 momentum
        df['h1_momentum'] = df['close_H1'].pct_change(5)
    else:
        # Fill with zeros if H1 data not available
        df['h1_trend_bias'] = 0.0
        df['h1_volatility_regime'] = 0.0
        df['h1_structure_strength'] = 0.0
        df['h1_momentum'] = 0.0

    return df


def engineer_h4_features(df: pd.DataFrame) -> pd.DataFrame:
    """Engineer H4 timeframe features for macro trend analysis"""
    if 'close_H4' in df.columns:
        # H4 macro trend
        df['h4_ema_200'] = df['close_H4'].ewm(span=200).mean()
        df['h4_macro_trend'] = (
            df['close_H4'] - df['h4_ema_200']) / df['close_H4']

        # H4 regime classification
        df['h4_rsi'] = calculate_rsi(df['close_H4'])
        df['h4_regime_classification'] = np.where(
            df['h4_rsi'] > 70, 1,  # Overbought
            np.where(df['h4_rsi'] < 30, -1, 0)  # Oversold, Neutral
        )

        # H4 volatility cycle
        df['h4_atr'] = calculate_atr(df[['high_H4', 'low_H4', 'close_H4']].rename(columns={
            'high_H4': 'high', 'low_H4': 'low', 'close_H4': 'close'
        }))
        df['h4_volatility_cycle'] = df['h4_atr'].rolling(
            20).mean() / df['h4_atr'].rolling(100).mean()
    else:
        df['h4_macro_trend'] = 0.0
        df['h4_regime_classification'] = 0.0
        df['h4_volatility_cycle'] = 1.0

    return df


def calculate_mtf_confluence(df: pd.DataFrame) -> pd.DataFrame:
    """Calculate multi-timeframe confluence indicators"""
    # Trend alignment across timeframes
    base_trend = np.sign(df['return_15m'])
    h1_trend = np.sign(df.get('h1_trend_bias', 0))
    h4_trend = np.sign(df.get('h4_macro_trend', 0))

    df['mtf_trend_alignment'] = (
        base_trend == h1_trend) & (h1_trend == h4_trend)
    df['mtf_trend_alignment'] = df['mtf_trend_alignment'].astype(int)

    # Structure confluence
    df['mtf_structure_confluence'] = (
        df['bos_close_confirm'] *
        (1 + df.get('h1_structure_strength', 0)) *
        (1 + abs(df.get('h4_macro_trend', 0)))
    )

    # Volatility synchronization
    base_vol = df['atr_norm']
    h1_vol = abs(df.get('h1_volatility_regime', 0))
    h4_vol = df.get('h4_volatility_cycle', 1)

    df['mtf_volatility_sync'] = (base_vol * h1_vol * h4_vol)

    return df


def generate_enhanced_labels(df: pd.DataFrame, lookahead: int = 8, threshold: float = 0.006) -> pd.DataFrame:
    """
    Generate enhanced labels with adaptive thresholds
    Updated: 0.6% threshold (was 0.15%) - more appropriate for H4/M15 timeframes
    Removed aggressive multi-timeframe downgrading that created too many HOLDs
    """
    df = df.copy()

    # Calculate future returns
    future_return = df['close'].shift(-lookahead) / df['close'] - 1

    # Base classification with better threshold
    df['label'] = 1  # Default neutral
    df.loc[future_return > threshold, 'label'] = 2  # Bullish
    df.loc[future_return < -threshold, 'label'] = 0  # Bearish

    # REMOVED: Multi-timeframe validation was too aggressive
    # It was downgrading too many signals to HOLD, causing 88% HOLD bias
    # The models should learn the multi-timeframe relationships themselves
    
    # Optional: Use ATR-based adaptive threshold (commented out for now)
    # if 'atr' in df.columns and 'close' in df.columns:
    #     adaptive_threshold = 2.0 * (df['atr'] / df['close'])
    #     df.loc[future_return > adaptive_threshold, 'label'] = 2
    #     df.loc[future_return < -adaptive_threshold, 'label'] = 0

    # Remove rows with NaN labels
    df = df.dropna(subset=['label'])

    return df


def extract_feature_matrix(features_df: pd.DataFrame, labels_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
    """Extract feature matrix and labels array"""

    # Define feature columns in order
    feature_columns = [
        'return_1m', 'return_5m', 'return_15m', 'sma_ratio', 'rsi_norm', 'atr_norm',
        'ob_bullish_exists', 'ob_bullish_size_atr', 'ob_bullish_displacement_atr',
        'ob_bearish_exists', 'ob_bearish_size_atr', 'ob_bearish_displacement_atr',
        'fvg_bullish_exists', 'fvg_bullish_depth_atr', 'fvg_bearish_exists', 'fvg_bearish_depth_atr',
        'bos_wick_confirm', 'bos_close_confirm', 'bos_dist_atr',
        'h1_trend_bias', 'h1_volatility_regime', 'h1_structure_strength', 'h1_momentum',
        'h4_macro_trend', 'h4_regime_classification', 'h4_volatility_cycle',
        'mtf_trend_alignment', 'mtf_structure_confluence', 'mtf_volatility_sync'
    ]

    # Create feature matrix
    feature_matrix = np.zeros((len(labels_df), len(feature_columns)))

    for i, col in enumerate(feature_columns):
        if col in labels_df.columns:
            feature_matrix[:, i] = labels_df[col].fillna(0).values
        else:
            print(f"Warning: Feature '{col}' not found, filling with zeros")

    # Extract labels
    labels_array = labels_df['label'].values.astype(np.int64)

    # Remove any rows with invalid labels
    valid_mask = ~np.isnan(labels_array)
    feature_matrix = feature_matrix[valid_mask]
    labels_array = labels_array[valid_mask]

    return feature_matrix, labels_array


def standardize_features(features: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Standardize features to zero mean and unit variance"""
    mean = features.mean(axis=0)
    std = features.std(axis=0)
    std[std == 0.0] = 1.0  # Avoid division by zero
    normalized = (features - mean) / std
    return normalized, mean, std


def train_enhanced_model(features: np.ndarray, labels: np.ndarray,
                         epochs: int = 60, batch_size: int = 512,
                         lr: float = 1e-3, seed: int = 42, use_amp: bool = True) -> Tuple[EnhancedSMC_MLP, float, float]:
    """
    Train the enhanced multi-timeframe SMC model
    
    GPU OPTIMIZATIONS:
    - Larger batch size (512 for GPU)
    - Mixed precision training (AMP)
    - Non-blocking transfers
    - Pinned memory
    """
    torch.manual_seed(seed)
    np.random.seed(seed)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Training on device: {device}")
    if device.type == 'cuda':
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        torch.backends.cudnn.benchmark = True  # Optimize for fixed input sizes

    # Convert to tensors with pinned memory for faster GPU transfer
    X = torch.tensor(features, dtype=torch.float32)
    y = torch.tensor(labels, dtype=torch.long)
    
    if device.type == 'cuda':
        X = X.pin_memory().to(device, non_blocking=True)
        y = y.pin_memory().to(device, non_blocking=True)
    else:
        X = X.to(device)
        y = y.to(device)

    # Initialize model
    model = EnhancedSMC_MLP(
        input_dim=features.shape[1],
        num_classes=len(np.unique(labels)),
        hidden_dims=[128, 96, 64, 32],
        dropout_p=0.3
    ).to(device)

    # Optimizer with stronger weight decay for regularization
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-4)
    
    # Warmup + ReduceLROnPlateau schedule for smoother training
    warmup_epochs = 5
    base_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=8, factor=0.5, verbose=True)

    # Loss function with class weights and label smoothing
    class_counts = np.bincount(labels)
    class_weights = len(labels) / (len(class_counts) * class_counts)
    class_weights = torch.tensor(
        class_weights, dtype=torch.float32, device=device)
    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)

    # Training loop with validation split
    n = X.shape[0]
    val_size = int(0.2 * n)
    train_size = n - val_size

    # Time-series split (last 20% for validation)
    X_train, X_val = X[:train_size], X[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]

    print(f"Training samples: {train_size}, Validation samples: {val_size}")

    best_val_loss = float('inf')
    patience_counter = 0
    
    # Mixed precision scaler for GPU
    scaler = torch.cuda.amp.GradScaler() if use_amp and device.type == 'cuda' else None

    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0

        # Shuffle training data
        perm = torch.randperm(train_size, device=device)
        X_train_shuf = X_train[perm]
        y_train_shuf = y_train[perm]

        for i in range(0, train_size, batch_size):
            batch_X = X_train_shuf[i:i+batch_size]
            batch_y = y_train_shuf[i:i+batch_size]

            optimizer.zero_grad()
            
            # Mixed precision training
            if scaler is not None:
                with torch.cuda.amp.autocast():
                    logits = model(batch_X)
                    loss = criterion(logits, batch_y)
                
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                logits = model(batch_X)
                loss = criterion(logits, batch_y)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

            train_loss += loss.item()
            train_correct += (logits.argmax(dim=1) == batch_y).sum().item()

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0

        with torch.no_grad():
            for i in range(0, val_size, batch_size):
                batch_X = X_val[i:i+batch_size]
                batch_y = y_val[i:i+batch_size]

                # Use AMP for validation too
                if scaler is not None:
                    with torch.cuda.amp.autocast():
                        logits = model(batch_X)
                        loss = criterion(logits, batch_y)
                else:
                    logits = model(batch_X)
                    loss = criterion(logits, batch_y)

                val_loss += loss.item()
                val_correct += (logits.argmax(dim=1) == batch_y).sum().item()

        # Calculate metrics
        train_acc = train_correct / train_size
        val_acc = val_correct / val_size
        avg_train_loss = train_loss / (train_size // batch_size + 1)
        avg_val_loss = val_loss / (val_size // batch_size + 1)

        # Learning rate scheduling with warmup
        if epoch < warmup_epochs:
            # Linear warmup
            warmup_factor = (epoch + 1) / warmup_epochs
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr * warmup_factor
        else:
            base_scheduler.step(avg_val_loss)

        # Early stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1

        if epoch % 10 == 0 or patience_counter >= 15:
            print(f"Epoch {epoch:3d}: Train Loss={avg_train_loss:.4f}, Train Acc={train_acc:.4f}, "
                  f"Val Loss={avg_val_loss:.4f}, Val Acc={val_acc:.4f}, LR={optimizer.param_groups[0]['lr']:.6f}")

        if patience_counter >= 15:
            print("Early stopping triggered")
            break

    # Final evaluation on full dataset
    model.eval()
    with torch.no_grad():
        logits = model(X)
        preds = logits.argmax(dim=1).cpu().numpy()
        accuracy = (preds == labels).mean()

        probs = torch.softmax(logits, dim=1).cpu().numpy()
        entropy = -(probs * np.log(probs + 1e-12)).sum(axis=1).mean()

    return model, accuracy, entropy


def walk_forward_validation_enhanced(features: np.ndarray, labels: np.ndarray,
                                     n_splits: int = 5, min_train_ratio: float = 0.3) -> Dict:
    """
    Enhanced walk-forward validation for time series data
    """
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

    n = len(features)
    min_train_size = int(n * min_train_ratio)

    fold_results = []

    for fold in range(n_splits):
        # Expanding window approach
        train_end = min_train_size + fold * ((n - min_train_size) // n_splits)
        test_start = train_end
        test_end = min(n, train_end + ((n - min_train_size) // n_splits))

        if test_start >= n or test_end <= test_start:
            break

        X_train = features[:train_end]
        y_train = labels[:train_end]
        X_test = features[test_start:test_end]
        y_test = labels[test_start:test_end]

        # Standardize features
        X_train_norm, mean, std = standardize_features(X_train)
        X_test_norm = (X_test - mean) / std

        # Train model for this fold
        model, _, _ = train_enhanced_model(
            X_train_norm, y_train,
            epochs=40, batch_size=256,
            lr=1e-3, seed=42 + fold
        )

        # Evaluate
        model.eval()
        with torch.no_grad():
            X_test_tensor = torch.tensor(X_test_norm, dtype=torch.float32)
            logits = model(X_test_tensor)
            preds = logits.argmax(dim=1).numpy()

        # Calculate metrics
        acc = accuracy_score(y_test, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_test, preds, average='weighted', zero_division=0
        )
        cm = confusion_matrix(y_test, preds)

        fold_results.append({
            'fold': fold,
            'train_size': len(y_train),
            'test_size': len(y_test),
            'accuracy': acc,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'confusion_matrix': cm.tolist()
        })

        print(
            f"Fold {fold}: Train={len(y_train)}, Test={len(y_test)}, Acc={acc:.4f}")

    # Aggregate results
    accuracies = [f['accuracy'] for f in fold_results]

    return {
        'folds': fold_results,
        'n_folds': len(fold_results),
        'accuracy_mean': np.mean(accuracies),
        'accuracy_std': np.std(accuracies),
        'accuracy_min': np.min(accuracies),
        'accuracy_max': np.max(accuracies)
    }


def export_enhanced_model(model: EnhancedSMC_MLP, feature_count: int, model_path: Path):
    """Export enhanced model to ONNX format"""
    model.eval()
    dummy_input = torch.randn(1, feature_count, dtype=torch.float32)

    torch.onnx.export(
        model,
        dummy_input,
        model_path,
        input_names=["features"],
        output_names=["logits"],
        dynamic_axes={"features": {0: "batch"}, "logits": {0: "batch"}},
        opset_version=11,
        do_constant_folding=True
    )

    print(f"Enhanced model exported to {model_path}")


def main():
    """Main execution pipeline"""
    print("=== Enhanced Multi-Timeframe SMC Pipeline ===")

    # Load and engineer features
    features, labels, symbols = load_and_engineer_multitf_features(DATA_PATH)

    print(f"\nDataset Summary:")
    print(f"Total samples: {len(features)}")
    print(f"Feature dimensions: {features.shape[1]}")
    print(f"Label distribution: {np.bincount(labels)}")
    print(f"Symbols: {len(set(symbols))} unique")

    # Standardize features
    features_norm, feature_mean, feature_std = standardize_features(features)

    # Train enhanced model
    print(f"\n=== Training Enhanced Model ===")
    model, accuracy, entropy = train_enhanced_model(
        features_norm, labels,
        epochs=60, batch_size=256, lr=1e-3
    )

    print(f"\nTraining Results:")
    print(f"Final Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"Average Entropy: {entropy:.4f}")

    # Walk-forward validation
    print(f"\n=== Walk-Forward Validation ===")
    wf_results = walk_forward_validation_enhanced(
        features_norm, labels, n_splits=5)

    print(f"\nOut-of-Sample Results:")
    print(
        f"Mean Accuracy: {wf_results['accuracy_mean']:.4f} Â± {wf_results['accuracy_std']:.4f}")
    print(
        f"Accuracy Range: [{wf_results['accuracy_min']:.4f}, {wf_results['accuracy_max']:.4f}]")

    # Export model
    OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)
    model_path = OUTPUT_ROOT / MODEL_FILENAME
    export_enhanced_model(model, features.shape[1], model_path)

    # Save metadata
    metadata = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "model_type": "EnhancedSMC_MLP",
        "feature_names": list(FEATURE_NAMES[:features.shape[1]]),
        "label_names": list(LABEL_NAMES),
        "training_samples": int(len(features)),
        "feature_dimensions": int(features.shape[1]),
        "symbols_count": len(set(symbols)),
        "feature_mean": feature_mean.tolist(),
        "feature_std": feature_std.tolist(),
        "final_accuracy": float(accuracy),
        "average_entropy": float(entropy),
        "oos_validation": wf_results,
        "architecture": {
            "hidden_dims": [128, 96, 64, 32],
            "dropout": 0.3,
            "total_parameters": sum(p.numel() for p in model.parameters())
        }
    }

    metadata_path = OUTPUT_ROOT.parent / "config" / METADATA_FILENAME
    metadata_path.parent.mkdir(parents=True, exist_ok=True)

    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)

    print(f"\nMetadata saved to: {metadata_path}")
    print(f"Enhanced Multi-Timeframe SMC Pipeline Complete!")

    return model, metadata


if __name__ == "__main__":
    model, metadata = main()
